{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07b6978d-1e8a-426e-8e46-cb03874ce22f",
   "metadata": {},
   "source": [
    "## Part 1: Create Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7610a0a5-bc94-479a-b77e-1e4e2d6997e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "homedir =  r'C:\\Users\\hongz\\Downloads\\Covid Exposure'\n",
    "\n",
    "import os\n",
    "os.chdir(homedir)\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "652c569c-56ca-4872-82e3-3e8f757c7ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dec63e57-a9df-4828-90b4-73fb2205acfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_zip = r'input\\transcripts\\sp500_transcripts.zip'\n",
    "\n",
    "# Interest Words\n",
    "words = {'Coronavirus', 'Corona virus', 'coronavirus', 'Covid-19', 'COVID-19', 'Covid19', 'COVID19', 'SARS-CoV-2', '2019-nCoV'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "49c4e198-f3ff-41fb-adeb-295325b4e288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompile Regular Expression\n",
    "regex_preprocess = re.compile(r'[^A-Za-z]') # remove non-alphabetic characters\n",
    "\n",
    "regex_findwords = re.compile(\n",
    "    r'\\b(?P<match>{0})\\b'.format(\n",
    "        '|'.join([regex_preprocess.sub('', x) for x in words])\n",
    "    ),\n",
    "    flags=re.IGNORECASE\n",
    ")\n",
    "\n",
    "regex = {\n",
    "    'preprocess': regex_preprocess,\n",
    "    'findwords': regex_findwords\n",
    "}\n",
    "\n",
    "output_file = r'output\\scored_dta.dta'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee186f8-2f57-4358-8a15-db2ee1b807a2",
   "metadata": {},
   "source": [
    "Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ca567639-8ceb-4d9d-a3be-cb1431acb65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_transcripts(file):\n",
    "    with zipfile.ZipFile(file, 'r') as trzip:\n",
    "        with trzip.open('sp500_transcripts.pkl') as infile:\n",
    "            df = pd.read_pickle(infile)\n",
    "    return df\n",
    "\n",
    "def score_transcripts(file,regex,outfile):\n",
    "    subdf = df.assign(\n",
    "        preprocessed = df['speech'].str.replace(regex['preprocess'], ''),\n",
    "        count = lambda x: x['preprocessed'].str.count(regex['findwords']),\n",
    "        length = lambda x: x['preprocessed'].str.split().str.len(),\n",
    "        normalized = lambda x: x['count'].div(x['length']).mul(100000)\n",
    "    )\n",
    "    subdf.drop(columns = ['preprocessed', 'speech'], inplace = True)\n",
    "    varlabs = {\n",
    "        count : 'Count of Covid-related words',\n",
    "        length : 'Total Words in Earning Calls',\n",
    "        normalized : (count/length)*100000,\n",
    "        event_date : 'Date of Earning Calls',\n",
    "        hqcountrycode : 'Country Code of Headquarters'\n",
    "    }\n",
    "    subdf.to_stata(outfile, write_index=False, variable_labels=varlabs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c05f262-d1d7-4cf0-a558-3b1e8b396495",
   "metadata": {},
   "source": [
    "Run Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cc74b3a7-a20f-4801-98fa-d153e63d09a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mscore_transcripts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_transcripts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtranscript_zip\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_file\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProcessed in\u001b[39m\u001b[38;5;124m'\u001b[39m, time\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m, time\u001b[38;5;241m.\u001b[39mgmtime(end\u001b[38;5;241m-\u001b[39mstart)), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin-sec\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[37], line 8\u001b[0m, in \u001b[0;36mscore_transcripts\u001b[1;34m(file, regex, outfile)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscore_transcripts\u001b[39m(file,regex,outfile):\n\u001b[1;32m----> 8\u001b[0m     subdf \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39massign(\n\u001b[0;32m      9\u001b[0m         preprocessed \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspeech\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mreplace(regex[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocess\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     10\u001b[0m         count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessed\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mcount(regex[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfindwords\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[0;32m     11\u001b[0m         length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessed\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39msplit()\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mlen(),\n\u001b[0;32m     12\u001b[0m         normalized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdiv(x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mmul(\u001b[38;5;241m100000\u001b[39m)\n\u001b[0;32m     13\u001b[0m     )\n\u001b[0;32m     14\u001b[0m     subdf\u001b[38;5;241m.\u001b[39mdrop(columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessed\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspeech\u001b[39m\u001b[38;5;124m'\u001b[39m], inplace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     15\u001b[0m     varlabs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     16\u001b[0m         count : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCount of Covid-related words\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     17\u001b[0m         length : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal Words in Earning Calls\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m         hqcountrycode : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCountry Code of Headquarters\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     21\u001b[0m     }\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "score_transcripts(\n",
    "    load_transcripts(transcript_zip),\n",
    "    regex,\n",
    "    output_file\n",
    ")\n",
    "end = time.time()\n",
    "print('Processed in', time.strftime('%M:%S', time.gmtime(end-start)), 'min-sec')\n",
    "\n",
    "df = load_transcripts(transcripts_zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54cc7260-9369-4949-800c-9bcd573eed5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hongz\\Downloads\\Covid Exposure\\path\\to\\transcript.zip\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Example to find the absolute path\n",
    "print(os.path.abspath('path/to/transcript.zip'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d00db13c-d067-45fd-b276-1553b24e01af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 476\n",
      "Arithmetic mean: 74.8755\n",
      "Sample variance: 98.7714\n",
      "Sample standard deviation: 9.9384\n",
      "Estimator (theta hat): 74.7185\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('C:/Users/hongz/Downloads/Salmon_Sample_6_24.csv')\n",
    "\n",
    "# Extract the lengths column (assuming the column name is 'length')\n",
    "lengths = data['Fish_Length_cm']\n",
    "\n",
    "# a) Number of salmon sampled\n",
    "num_samples = len(lengths)\n",
    "\n",
    "# b) Arithmetic mean of the lengths\n",
    "mean_length = lengths.mean()\n",
    "\n",
    "# c) Sample variance of the lengths\n",
    "sample_variance = lengths.var(ddof=1)  # ddof=1 specifies that we are calculating sample variance\n",
    "\n",
    "# d) Sample standard deviation of the lengths\n",
    "sample_std_dev = lengths.std(ddof=1)  # ddof=1 specifies that we are calculating sample standard deviation\n",
    "\n",
    "# Print the results\n",
    "print(f\"Number of samples: {num_samples}\")\n",
    "print(f\"Arithmetic mean: {mean_length:.4f}\")\n",
    "print(f\"Sample variance: {sample_variance:.4f}\")\n",
    "print(f\"Sample standard deviation: {sample_std_dev:.4f}\")\n",
    "\n",
    "# Number of samples\n",
    "n = len(lengths)\n",
    "\n",
    "# Calculate the estimator\n",
    "theta_hat = (1 / (n + 1)) * lengths.sum()\n",
    "\n",
    "# Print the result rounded to 4 decimal places\n",
    "print(f\"Estimator (theta hat): {theta_hat:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6acff1be-374b-4961-96df-57c8ac55d3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed in 00:08 min-sec\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "\n",
    "# Set the home directory\n",
    "homedir = r'C:\\Users\\hongz\\Downloads\\Covid Exposure'\n",
    "os.chdir(homedir)\n",
    "\n",
    "# Function to load transcripts from a zip file\n",
    "def load_transcripts(file):\n",
    "    with zipfile.ZipFile(file, 'r') as trzip:\n",
    "        with trzip.open('sp500_transcripts.pkl') as infile:\n",
    "            df = pd.read_pickle(infile)\n",
    "    return df\n",
    "\n",
    "# Function to score transcripts and save the result to a Stata file\n",
    "def score_transcripts(df, regex, outfile):\n",
    "    subdf = df.assign(\n",
    "        preprocessed = df['speech'].str.replace(regex['preprocess'], '', regex=True),\n",
    "        count = lambda x: x['preprocessed'].str.count(regex['findwords']),\n",
    "        length = lambda x: x['preprocessed'].str.split().str.len(),\n",
    "        normalized = lambda x: x['count'].div(x['length']).mul(100000)\n",
    "    )\n",
    "    subdf.drop(columns = ['preprocessed', 'speech'], inplace = True)\n",
    "    varlabs = {\n",
    "        'count': 'Count of Covid-related words',\n",
    "        'length': 'Total Words in Earning Calls',\n",
    "        'normalized': 'Normalized Count of Covid-related words per 100,000 words',\n",
    "        'event_date': 'Date of Earning Calls',\n",
    "        'hqcountrycode': 'Country Code of Headquarters'\n",
    "    }\n",
    "    subdf.to_stata(outfile, write_index=False, variable_labels=varlabs)\n",
    "\n",
    "# File paths and regex dictionary\n",
    "transcript_zip = r'input\\transcripts\\sp500_transcripts.zip'  # Update this with the actual path\n",
    "output_file = r'output\\scored_dta.dta'  # Update this with the desired output file path\n",
    "regex = {\n",
    "    'preprocess': r'[^A-Za-z\\s]',\n",
    "    'findwords': r'\\b(?:example|words|to|match)\\b'\n",
    "}\n",
    "\n",
    "# Timing the process\n",
    "start = time.time()\n",
    "\n",
    "# Loading transcripts and scoring them\n",
    "df = load_transcripts(transcript_zip)\n",
    "score_transcripts(df, regex, output_file)\n",
    "\n",
    "end = time.time()\n",
    "print('Processed in', time.strftime('%M:%S', time.gmtime(end-start)), 'min-sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "de8e22ac-cf6a-4f96-ab99-5f2e3cef6e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed in 00:07 min-sec\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "\n",
    "# Set the home directory\n",
    "homedir = r'C:\\Users\\hongz\\Downloads\\Covid Exposure'\n",
    "os.chdir(homedir)\n",
    "\n",
    "# Function to load transcripts from a zip file\n",
    "def load_transcripts(file):\n",
    "    with zipfile.ZipFile(file, 'r') as trzip:\n",
    "        with trzip.open('sp500_transcripts.pkl') as infile:\n",
    "            df = pd.read_pickle(infile)\n",
    "    return df\n",
    "\n",
    "# Function to score transcripts and save the result to a Stata file\n",
    "def score_transcripts(df, regex, outfile):\n",
    "    df['preprocessed'] = df['speech'].str.replace(regex['preprocess'], '', regex=True)\n",
    "    df['count'] = df['preprocessed'].str.count(regex['findwords'])\n",
    "    df['length'] = df['preprocessed'].str.split().str.len()\n",
    "    df['normalized'] = df['count'].div(df['length']).mul(100000)\n",
    "\n",
    "    subdf = df.drop(columns=['preprocessed', 'speech'])\n",
    "\n",
    "    varlabs = {\n",
    "        'count': 'Count of Covid-related words',\n",
    "        'length': 'Total Words in Earning Calls',\n",
    "        'normalized': 'Normalized Count of Covid-related words per 100,000 words',\n",
    "        'event_date': 'Date of Earning Calls',\n",
    "        'hqcountrycode': 'Country Code of Headquarters'\n",
    "    }\n",
    "\n",
    "    subdf.to_stata(outfile, write_index=False, variable_labels=varlabs)\n",
    "\n",
    "# File paths and regex dictionary\n",
    "transcript_zip = r'input\\transcripts\\sp500_transcripts.zip'\n",
    "output_file = r'output\\scored_dta.dta'\n",
    "regex = {\n",
    "    'preprocess': r'[^A-Za-z\\s]',\n",
    "    'findwords': r'\\b(?:example|words|to|match)\\b'\n",
    "}\n",
    "\n",
    "# Timing the process\n",
    "start = time.time()\n",
    "\n",
    "# Loading transcripts and scoring them\n",
    "df = load_transcripts(transcript_zip)\n",
    "score_transcripts(df, regex, output_file)\n",
    "\n",
    "end = time.time()\n",
    "print('Processed in', time.strftime('%M:%S', time.gmtime(end-start)), 'min-sec')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5630536b-2cb0-4ee8-8577-fa27eb384ddf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
